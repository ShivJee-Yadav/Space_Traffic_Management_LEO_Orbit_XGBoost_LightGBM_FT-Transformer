{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaa5e8d",
   "metadata": {},
   "source": [
    "\n",
    "# IGOM-ML â€” LightGBM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required Python packages (run in Colab)\n",
    "!pip install -q lightgbm shap optuna joblib pandas numpy scikit-learn pyarrow matplotlib\n",
    "\n",
    "# Basic imports and seed\n",
    "import os, random\n",
    "import numpy as np, pandas as pd\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Setup complete. Python version:\", os.sys.version.split(\"\\n\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mount Google Drive (uncomment when running in Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set base path in Drive where dataset and outputs will be stored.\n",
    "DRIVE_BASE = '/content/drive/MyDrive/IGOM_ML'  # change as needed\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "print(\"DRIVE_BASE set to:\", DRIVE_BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (Parquet recommended). If you uploaded directly to session, change the path.\n",
    "import os\n",
    "default_path = os.path.join(DRIVE_BASE, 'fused_replay.parquet')\n",
    "if os.path.exists(default_path):\n",
    "    df = pd.read_parquet(default_path)\n",
    "    print(\"Loaded fused_replay.parquet from Drive, rows:\", len(df))\n",
    "else:\n",
    "    print(\"No dataset at\", default_path)\n",
    "    # Fallback: create a tiny synthetic dataset for demo purposes (remove for real runs)\n",
    "    print(\"Creating synthetic demo dataset (for code flow only). Replace with your real fused_replay.parquet\")\n",
    "    n = 2000\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    df = pd.DataFrame({\n",
    "        'cdm_id': np.arange(n),\n",
    "        'object_A_id': rng.randint(1000,2000,n),\n",
    "        'object_B_id': rng.randint(2000,3000,n),\n",
    "        'object_pair_id': [f\"{a}_{b}\" for a,b in zip(rng.randint(1000,2000,n), rng.randint(2000,3000,n))],\n",
    "        'timestamp': pd.Timestamp('2025-01-01') + pd.to_timedelta(rng.randint(0,86400*30,n), unit='s'),\n",
    "        # physics-like features\n",
    "        'range': rng.uniform(50,5000,n),  # km\n",
    "        'rel_vx': rng.normal(0,0.5,n),\n",
    "        'rel_vy': rng.normal(0,0.5,n),\n",
    "        'rel_vz': rng.normal(0,0.2,n),\n",
    "        'radar_snr': rng.uniform(0,30,n),\n",
    "        'num_tracks': rng.randint(1,8,n),\n",
    "        'catalog_age_hours': rng.uniform(0,720,n),\n",
    "        # target (rare events)\n",
    "        'collision_label': (rng.rand(n) < 0.02).astype(int)\n",
    "    })\n",
    "    print(\"Synthetic dataset created. Positive rate:\", df['collision_label'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84417071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic feature engineering examples. Extend these based on your available fields.\n",
    "df['rel_speed'] = np.sqrt(df['rel_vx']**2 + df['rel_vy']**2 + df['rel_vz']**2)\n",
    "# convert range from km to meters for intuition\n",
    "df['range_m'] = df['range'] * 1000.0\n",
    "# interaction features\n",
    "df['range_over_tracks'] = df['range_m'] / (df['num_tracks'] + 1e-6)\n",
    "df['dist_times_snr'] = df['range_m'] * (df['radar_snr'] + 1e-6)\n",
    "# temporal features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
    "df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
    "\n",
    "# Fill missing values and choose features\n",
    "df.fillna(-999, inplace=True)\n",
    "feature_cols = ['range_m','rel_speed','radar_snr','num_tracks','catalog_age_hours',\n",
    "                'range_over_tracks','dist_times_snr','hour_sin','hour_cos']\n",
    "target_col = 'collision_label'\n",
    "\n",
    "print(\"Features ready. Example row:\")\n",
    "display(df[feature_cols + [target_col]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train LightGBM with GroupKFold to avoid leakage across object pairs.\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col].astype(int)\n",
    "groups = df['object_pair_id']\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "models = []\n",
    "val_preds = []\n",
    "val_trues = []\n",
    "fold = 0\n",
    "for train_idx, val_idx in gkf.split(X, y, groups=groups):\n",
    "    fold += 1\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "    params = {\n",
    "        'objective':'binary',\n",
    "        'metric':'auc',\n",
    "        'learning_rate':0.05,\n",
    "        'num_leaves':63,\n",
    "        'max_depth':7,\n",
    "        'min_data_in_leaf':20,\n",
    "        'feature_fraction':0.8,\n",
    "        'bagging_fraction':0.8,\n",
    "        'bagging_freq':5,\n",
    "        'seed': SEED + fold,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "    bst = lgb.train(params, dtrain, valid_sets=[dval], num_boost_round=2000, early_stopping_rounds=100, verbose_eval=100)\n",
    "    p_val = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "    print(f\"Fold {fold} ROC-AUC: {roc_auc_score(y_val, p_val):.4f}, PR-AUC: {average_precision_score(y_val, p_val):.4f}, Brier: {brier_score_loss(y_val, p_val):.4f}\")\n",
    "    models.append(bst)\n",
    "    val_preds.append(pd.Series(p_val, index=val_idx))\n",
    "    val_trues.append(pd.Series(y_val.values, index=val_idx))\n",
    "\n",
    "# aggregate OOF predictions\n",
    "import pandas as pd\n",
    "oof_pred = pd.concat(val_preds).sort_index()\n",
    "oof_true = pd.concat(val_trues).sort_index()\n",
    "print(\"\\nOverall OOF ROC-AUC:\", roc_auc_score(oof_true, oof_pred))\n",
    "print(\"Overall OOF PR-AUC:\", average_precision_score(oof_true, oof_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1804e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Probability calibration using sklearn's CalibratedClassifierCV on a pooled LightGBM wrapper.\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Create a wrapper classifier that averages predictions from the LightGBM models\n",
    "class LGBEnsembleWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        preds = np.column_stack([m.predict(X, num_iteration=m.best_iteration) for m in self.models])\n",
    "        mean = preds.mean(axis=1)\n",
    "        # return two-column probability matrix\n",
    "        return np.vstack([1-mean, mean]).T\n",
    "\n",
    "# Fit calibrator on a holdout split (here we reuse a fraction of the dataset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_cal, X_cal, y_train_cal, y_cal = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "ens = LGBEnsembleWrapper(models)\n",
    "calibrator = CalibratedClassifierCV(base_estimator=ens, method='isotonic', cv='prefit')\n",
    "# CalibratedClassifierCV with cv='prefit' expects base estimator already \"fit\" - our wrapper simply uses models\n",
    "calibrator.fit(X_cal, y_cal)\n",
    "# Evaluate calibration\n",
    "probs_cal = calibrator.predict_proba(X_cal)[:,1]\n",
    "from sklearn.metrics import brier_score_loss\n",
    "print(\"Calibration set Brier score:\", brier_score_loss(y_cal, probs_cal))\n",
    "# Save calibrator and wrapper\n",
    "artifact = {'models': models, 'features': feature_cols, 'calibrator': calibrator}\n",
    "joblib.dump(artifact, os.path.join(DRIVE_BASE, 'lgb_ensemble_artifact.joblib'))\n",
    "print(\"Saved artifact to Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHAP explainability for one model (tree explainer). Use ensemble average for robustness.\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the first model's TreeExplainer for speed. For production, aggregate SHAP across ensemble or use linear approximation.\n",
    "explainer = shap.TreeExplainer(models[0])\n",
    "sample_idx = X.sample(min(100, len(X)), random_state=SEED).index\n",
    "shap_vals = explainer.shap_values(X.loc[sample_idx])\n",
    "print(\"Displaying SHAP summary plot (may take a moment)...\")\n",
    "shap.summary_plot(shap_vals, X.loc[sample_idx], show=True)\n",
    "\n",
    "# Example: generate one-line reason for a single row\n",
    "def one_line_reason(row, shap_vals_row, feature_names, topk=3):\n",
    "    idxs = np.argsort(-np.abs(shap_vals_row))[:topk]\n",
    "    parts = []\n",
    "    for i in idxs:\n",
    "        fname = feature_names[i]\n",
    "        val = row[fname]\n",
    "        direction = \"high\" if shap_vals_row[i] > 0 else \"low\"\n",
    "        parts.append(f\"{direction} {fname}={val:.2f}\")\n",
    "    return \"Impact: \" + \", \".join(parts)\n",
    "\n",
    "# demo\n",
    "i = X.sample(1, random_state=SEED).index[0]\n",
    "row = X.loc[i]\n",
    "shap_row = explainer.shap_values(row.values.reshape(1,-1))[0]\n",
    "print(\"One-line reason example:\", one_line_reason(row, shap_row, feature_cols, topk=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b735312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Demo prediction API-like function using saved artifact\n",
    "import joblib\n",
    "art = joblib.load(os.path.join(DRIVE_BASE, 'lgb_ensemble_artifact.joblib'))\n",
    "models_loaded = art['models']\n",
    "features_loaded = art['features']\n",
    "calib = art['calibrator']\n",
    "\n",
    "def predict_event(feature_dict):\n",
    "    # feature_dict maps feature name -> value\n",
    "    x = pd.DataFrame([feature_dict])[features_loaded].fillna(-999)\n",
    "    # ensemble mean and std\n",
    "    preds = np.column_stack([m.predict(x, num_iteration=m.best_iteration) for m in models_loaded])\n",
    "    prob_mean = preds.mean(axis=1)[0]\n",
    "    prob_std = preds.std(axis=1)[0]\n",
    "    prob_cal = calib.predict_proba(x)[:,1][0]\n",
    "    return {'prob_mean': float(prob_mean), 'prob_std': float(prob_std), 'prob_calibrated': float(prob_cal)}\n",
    "\n",
    "# Demo with a random row\n",
    "sample = X.sample(1, random_state=SEED).iloc[0].to_dict()\n",
    "print(\"Demo prediction:\", predict_event(sample))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
